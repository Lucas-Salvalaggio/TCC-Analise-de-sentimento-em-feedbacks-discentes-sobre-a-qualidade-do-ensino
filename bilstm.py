# -*- coding: utf-8 -*-
"""bilstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZtS3X_gxbAlj5p32AUQwaWGw-IqzJo56

**Bidirectional LSTM**

AplicaÃ§Ã£o do ``neuralmind/bert-large-portuguese-cased`` na base de dados classificada manualmente

---
"""

# ==========================================
# ðŸ“¦ 1. IMPORTS E CONFIGURAÃ‡Ã•ES INICIAIS
# ==========================================
import os
import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.optim import AdamW
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import label_binarize
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_curve, auc
)
# ðŸŸ© Import oficial do scikit-learn
from sklearn.utils.class_weight import compute_class_weight

from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoConfig,
    TrainingArguments,
    Trainer
)
from datasets import Dataset
from google.colab import files

# ==========================================
# âš™ï¸ 2. CONFIGURAÃ‡Ã•ES GERAIS
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

EPOCHS = 5
BATCH_SIZE = 16
LEARNING_RATE = 2e-5
MAX_LEN = 128
N_SPLITS = 5
DROPOUT = 0.4
MODEL_NAME = "neuralmind/bert-base-portuguese-cased"

print("ðŸ§  Modelo:", MODEL_NAME)
print("ðŸ’» Dispositivo:", DEVICE)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# ==========================================
# ðŸ“ 3. CARREGAMENTO DO DATASET
# ==========================================
uploaded = files.upload()
data_path = list(uploaded.keys())[0]

if data_path.endswith(".xlsx"):
    df = pd.read_excel(data_path)
else:
    df = pd.read_csv(data_path)

df = df[["text", "label"]].dropna()
df["label"] = df["label"].astype(int)
df["text"] = df["text"].astype(str)

print("\nðŸ“Š DistribuiÃ§Ã£o original das classes:")
print(df["label"].value_counts())

# ==========================================
# ðŸ§© 4. CLASSE DO MODELO: BERT + BiLSTM + ATENÃ‡ÃƒO
# ==========================================
class BertBiLSTMAttention(nn.Module):
    # ðŸŸ© Novo parÃ¢metro para receber labels de treino
    def __init__(self, model_name, num_labels=3, lstm_hidden=256, dropout=0.4, labels_for_weights=None):
        super().__init__()

        self.config = AutoConfig.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name, config=self.config)
        hidden_size = self.config.hidden_size
        self.lstm_hidden = lstm_hidden

        # ðŸ”¹ BiLSTM bidirecional
        self.bilstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=lstm_hidden,
            num_layers=1,
            batch_first=True,
            bidirectional=True
        )

        # ðŸ”¹ AtenÃ§Ã£o e saÃ­da final
        self.attention = nn.Linear(lstm_hidden * 2, 1)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(lstm_hidden * 2, num_labels)

        # ðŸŸ© CÃ¡lculo de pesos conforme documentaÃ§Ã£o do scikit-learn
        if labels_for_weights is not None:
            labels_for_weights = np.asarray(labels_for_weights, dtype=int)
            classes_arr = np.array([0, 1, 2])  # define todas as classes
            weights = compute_class_weight(
                class_weight="balanced",
                classes=classes_arr,
                y=labels_for_weights
            )
            weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)
            print(f"âš–ï¸ Pesos das classes aplicados: {weights.tolist()}")
            self.criterion = nn.CrossEntropyLoss(weight=weights)
        else:
            self.criterion = nn.CrossEntropyLoss()

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        seq_out = bert_out.last_hidden_state
        lstm_out, _ = self.bilstm(seq_out)
        attn_scores = self.attention(lstm_out).squeeze(-1)
        attn_weights = torch.softmax(
            attn_scores.masked_fill(attention_mask == 0, -1e9),
            dim=1
        ).unsqueeze(-1)
        weighted = torch.sum(attn_weights * lstm_out, dim=1)
        x = self.dropout(weighted)
        logits = self.classifier(x)

        loss = None
        if labels is not None:
            loss = self.criterion(logits, labels)

        return {"loss": loss, "logits": logits}

# ==========================================
# ðŸ”„ 5. TREINAMENTO COM STRATIFIED K-FOLD
# ==========================================
skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)
accuracy_list, precision_list, recall_list, f1_list = [], [], [], []

classes = np.array([0, 1, 2])
labels_names = ["Negativo", "Neutro", "Positivo"]
cm_total = np.zeros((len(classes), len(classes)), dtype=int)
all_labels, all_probs = [], []

def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN
    )

# ==========================================
# ðŸ” LOOP DOS FOLDS
# ==========================================
for fold, (train_idx, val_idx) in enumerate(skf.split(df["text"], df["label"])):
    print(f"\n===== ðŸ§© FOLD {fold+1}/{N_SPLITS} =====")

    train_df = df.iloc[train_idx]
    val_df = df.iloc[val_idx]

    # ðŸŸ© CÃ¡lculo de pesos balanceados por fold (documentaÃ§Ã£o oficial)
    y_train = np.asarray(train_df["label"].values, dtype=int)
    classes_arr = np.array([0, 1, 2])
    weights = compute_class_weight(
        class_weight="balanced",
        classes=classes_arr,
        y=y_train
    )
    weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)
    print(f"âš–ï¸ Pesos (fold {fold+1}):", weights.tolist())

    # CriaÃ§Ã£o dos datasets
    train_dataset = Dataset.from_pandas(train_df)
    val_dataset = Dataset.from_pandas(val_df)

    train_dataset = train_dataset.map(tokenize, batched=True)
    val_dataset = val_dataset.map(tokenize, batched=True)

    train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

    # ðŸŸ© Modelo recebe labels de treino para calcular a loss ponderada
    model = BertBiLSTMAttention(
        MODEL_NAME,
        num_labels=3,
        lstm_hidden=256,
        dropout=DROPOUT,
        labels_for_weights=y_train
    ).to(DEVICE)

    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    training_args = TrainingArguments(
        output_dir=f"./results_fold_{fold}",
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="no",
        logging_dir=f"./logs_fold_{fold}",
        logging_steps=100,
        disable_tqdm=False,
        report_to="none"
    )

    def compute_metrics(pred):
        labels = pred.label_ids
        preds = np.argmax(pred.predictions, axis=1)
        acc = accuracy_score(labels, preds)
        prec = precision_score(labels, preds, average="weighted", zero_division=0)
        rec = recall_score(labels, preds, average="weighted", zero_division=0)
        f1 = f1_score(labels, preds, average="weighted", zero_division=0)
        return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    # AvaliaÃ§Ã£o
    preds_output = trainer.predict(val_dataset)
    logits = preds_output.predictions
    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()
    preds = np.argmax(logits, axis=1)
    labels = preds_output.label_ids

    acc = accuracy_score(labels, preds)
    prec = precision_score(labels, preds, average="weighted", zero_division=0)
    rec = recall_score(labels, preds, average="weighted", zero_division=0)
    f1 = f1_score(labels, preds, average="weighted", zero_division=0)

    accuracy_list.append(acc)
    precision_list.append(prec)
    recall_list.append(rec)
    f1_list.append(f1)

    print(f"Fold {fold+1} â†’ AcurÃ¡cia: {acc:.4f}, PrecisÃ£o: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}")

    cm = confusion_matrix(labels, preds, labels=classes)
    cm_total += cm

    all_labels.extend(labels)
    all_probs.extend(probs)

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=labels_names, yticklabels=labels_names)
    plt.title(f"Matriz de ConfusÃ£o - Fold {fold+1}")
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()

# ==========================================
# ðŸ“ˆ 6. RESULTADOS MÃ‰DIOS
# ==========================================
print("\n===== ðŸ“Š RESULTADOS MÃ‰DIOS =====")
print(f"AcurÃ¡cia MÃ©dia: {np.mean(accuracy_list):.4f}")
print(f"PrecisÃ£o MÃ©dia: {np.mean(precision_list):.4f}")
print(f"Recall MÃ©dio: {np.mean(recall_list):.4f}")
print(f"F1-Score MÃ©dio: {np.mean(f1_list):.4f}")

plt.figure(figsize=(6,5), dpi=200)
sns.set_theme(style="white", context="paper")
sns.heatmap(cm_total, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels_names, yticklabels=labels_names)
plt.title(" Matriz de ConfusÃ£o Agregada (5 Folds)")
plt.xlabel("Predito")
plt.ylabel("Real")
plt.tight_layout()
plt.show()

# =========================
# CÃ©lula 0 â€” Setup estÃ¡vel (GPU CUDA 12.6)
# =========================
# 1) Remover pacotes que causam conflito / restos de instalaÃ§Ãµes
!pip uninstall -y torch torchvision torchaudio xformers flash-attn triton bitsandbytes cudf-cu12 pylibcudf-cu12 cuml-cu12 fastai -q
!bash -lc 'rm -rf /usr/local/lib/python3.12/dist-packages/*~orch* || true'

# 2) Instalar PyTorch coerente com o Colab (CUDA 12.6)
#    Se estiver SEM GPU, veja a cÃ©lula alternativa mais abaixo (CPU).
!pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu126 \
  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 -q

# 3) Hugging Face + deps compatÃ­veis (datasets pede pyarrow>=21)
!pip install --no-cache-dir "pyarrow>=21,<22" -q
!pip install --no-cache-dir -U transformers==4.45.2 datasets==3.0.2 accelerate scikit-learn seaborn matplotlib -q

# 4) Imports
import os, gc, random
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
)
from sklearn.utils.class_weight import compute_class_weight

from transformers import AutoTokenizer, AutoModel, AutoConfig, TrainingArguments, Trainer
from datasets import Dataset
from torch import nn
from torch.optim import AdamW
from google.colab import files

# 5) Reprodutibilidade
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")



print("âœ… Setup completo!")
print("Torch        :", torch.__version__)
import transformers, datasets, pyarrow
print("Transformers :", transformers.__version__)
print("Datasets     :", datasets.__version__)
print("PyArrow      :", pyarrow.__version__)
print("Device       :", DEVICE)

# =========================
# CÃ©lula 1 â€” Upload do dataset
# =========================
print("FaÃ§a upload do arquivo CSV/Excel (ex: feedbacks_standardized.csv) com colunas: text, label (0/1/2)")
uploaded = files.upload()
fname = list(uploaded.keys())[0]

if fname.lower().endswith((".xlsx", ".xls")):
    df = pd.read_excel(fname)
else:
    df = pd.read_csv(fname)

# garantir colunas
assert "text" in df.columns and "label" in df.columns, "O arquivo precisa ter colunas 'text' e 'label' (0,1,2)."
df = df[["text","label"]].dropna().reset_index(drop=True)
df["label"] = df["label"].astype(int)
# Ensure the 'text' column is string type
df['text'] = df['text'].astype(str)
print("Linhas:", len(df))
print(df["label"].value_counts())
display(df.head())

# =========================
# CÃ©lula 2 â€” HiperparÃ¢metros principais
# =========================
MODEL_NAME = "neuralmind/bert-large-portuguese-cased"  # BERTimbau-large
NUM_LABELS = 3
MAX_LEN = 128
EPOCHS = 5
BATCH_SIZE = 16   # se OOM, reduzir para 8
LR = 2e-5
WEIGHT_DECAY = 0.01
DROPOUT = 0.4
N_SPLITS = 5

print(f"Modelo: {MODEL_NAME}\nepochs={EPOCHS}, batch_size={BATCH_SIZE}, lr={LR}, max_len={MAX_LEN}")

# =========================
# CÃ©lula 3 â€” Definir o modelo hÃ­brido PyTorch
# =========================
class BertBiLSTMAttention(nn.Module):
    def __init__(self, model_name, num_labels=3, lstm_hidden=256, dropout=0.4, labels_for_weights=None):
        super().__init__()
        # Carrega configuraÃ§Ã£o e backbone BERT (somente uma vez por criaÃ§Ã£o)
        self.config = AutoConfig.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name, config=self.config)
        hidden_size = self.config.hidden_size  # 1024 para large
        self.lstm_hidden = lstm_hidden

        # Bidirectional LSTM (usa saÃ­da de token do BERT como input)
        self.bilstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=lstm_hidden,
            num_layers=1,
            batch_first=True,
            bidirectional=True
        )

        # ðŸŸ© CÃ¡lculo dos pesos balanceados das classes
        if labels_for_weights is not None:
            classes_arr = np.unique(labels_for_weights)
            weights = compute_class_weight(
                class_weight="balanced",
                classes=classes_arr,
                y=labels_for_weights
            )
            weights = torch.tensor(weights, dtype=torch.float)
            print("âš–ï¸ Pesos das classes:", weights.tolist())
            self.criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE))
        else:
            self.criterion = nn.CrossEntropyLoss()

        # AtenÃ§Ã£o simples: projeta cada hidden state para um escalar, aplica softmax no tempo
        self.attention = nn.Linear(lstm_hidden * 2, 1)

        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(lstm_hidden * 2, num_labels)

        # loss
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        # BERT outputs: last_hidden_state (batch, seq_len, hidden_size)
        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = bert_out.last_hidden_state  # (B, T, H)
        # pass through BiLSTM
        lstm_out, _ = self.bilstm(sequence_output)  # (B, T, 2*hidden)
        # attention scores
        attn_scores = self.attention(lstm_out).squeeze(-1)  # (B, T)
        attn_weights = torch.softmax(attn_scores.masked_fill(attention_mask == 0, -1e9), dim=1).unsqueeze(-1)  # (B,T,1)
        # weighted sum
        weighted = torch.sum(attn_weights * lstm_out, dim=1)  # (B, 2*hidden)
        x = self.dropout(weighted)
        logits = self.classifier(x)  # (B, num_labels)

        loss = None
        if labels is not None:
            loss = self.criterion(logits, labels)

        # Trainer expects either (loss, logits, ...) or ModelOutput. We return as dict.
        output = {"loss": loss, "logits": logits}
        return output

# =========================
# CÃ©lula 4 â€” Tokenizer e funÃ§Ã£o utilitÃ¡ria
# =========================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_batch(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=MAX_LEN)

# =========================
# CÃ©lula 5 â€” 5-fold stratified training loop com Trainer
# =========================
skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)

fold_results = []
all_conf_matrices = []

# ðŸ”¹ Acumuladores para matriz agregada e para ROC
classes = [0, 1, 2]
labels_names = ["Negativo", "Neutro", "Positivo"]
cm_total = np.zeros((len(classes), len(classes)), dtype=int)
all_labels = []
all_probs = []


for fold, (train_idx, val_idx) in enumerate(skf.split(df["text"], df["label"]), start=1):
    print(f"\n===== FOLD {fold}/{N_SPLITS} =====")

    # Subsets
    train_df = df.iloc[train_idx].reset_index(drop=True)
    val_df = df.iloc[val_idx].reset_index(drop=True)

    # HuggingFace datasets
    hf_train = Dataset.from_pandas(train_df)
    hf_val = Dataset.from_pandas(val_df)

    # Tokenize
    hf_train = hf_train.map(tokenize_batch, batched=True, remove_columns=["text"])
    hf_val = hf_val.map(tokenize_batch, batched=True, remove_columns=["text"])

    # rename label column to 'labels' required by Trainer's default data collator
    hf_train = hf_train.rename_column("label", "labels")
    hf_val = hf_val.rename_column("label", "labels")

    hf_train.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
    hf_val.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

    # instantiate model (fresh weights)
    model = BertBiLSTMAttention(MODEL_NAME, num_labels=NUM_LABELS, lstm_hidden=256, dropout=DROPOUT, labels_for_weights=train_df["label"].values)
    model.to(DEVICE)

    # TrainingArguments
    output_dir = f"./bert_bilstm_attn_fold{fold}"
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LR,
        weight_decay=WEIGHT_DECAY,
        eval_strategy="epoch",
        save_strategy="no",
        logging_dir=f"{output_dir}/logs",
        logging_steps=50,
        report_to="none",
        seed=SEED,
    )

    # compute_metrics for trainer
    def compute_metrics(eval_pred):
        logits = eval_pred.predictions
        labels = eval_pred.label_ids
        preds = np.argmax(logits, axis=1)
        acc = accuracy_score(labels, preds)
        prec_macro = precision_score(labels, preds, average="macro", zero_division=0)
        rec_macro = recall_score(labels, preds, average="macro", zero_division=0)
        f1_macro = f1_score(labels, preds, average="macro", zero_division=0)
        f1_weighted = f1_score(labels, preds, average="weighted", zero_division=0)
        return {
            "accuracy": acc,
            "precision_macro": prec_macro,
            "recall_macro": rec_macro,
            "f1_macro": f1_macro,
            "f1_weighted": f1_weighted
        }

    # Trainer (the Trainer will call our model and get dict with loss/logits)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=hf_train,
        eval_dataset=hf_val,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # Train
    trainer.train()

    # Evaluate / predict on validation set
    out = trainer.predict(hf_val)
    logits = out.predictions
    y_true = out.label_ids
    y_pred = np.argmax(logits, axis=1)

    # metrics
    acc = accuracy_score(y_true, y_pred)
    prec_macro = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec_macro = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1_macro = f1_score(y_true, y_pred, average="macro", zero_division=0)
    f1_weighted = f1_score(y_true, y_pred, average="weighted", zero_division=0)

    print(f"Fold {fold} â€” acc: {acc:.4f}, f1_macro: {f1_macro:.4f}, f1_weighted: {f1_weighted:.4f}")

    # Guardar para ROC agregada
    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()  # converte logits em probabilidades
    all_labels.extend(y_true)
    all_probs.extend(probs)


    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])
    all_conf_matrices.append(cm)

    fold_results.append({
        "fold": fold,
        "accuracy": acc,
        "precision_macro": float(prec_macro),
        "recall_macro": float(rec_macro),
        "f1_macro": float(f1_macro),
        "f1_weighted": float(f1_weighted)
    })

    # Plot confusion matrix
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Neg","Neu","Pos"], yticklabels=["Neg","Neu","Pos"])
    plt.title(f"Confusion Matrix â€” Fold {fold}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    # cleanup
    del model
    del trainer
    torch.cuda.empty_cache()
    gc.collect()

# =========================
# CÃ©lula 6 â€” Resumo e mÃ©tricas agregadas
# =========================
df_folds = pd.DataFrame(fold_results)
display(df_folds)

print("MÃ©dias das mÃ©tricas (mean Â± std):")
print(df_folds[["accuracy","f1_macro","f1_weighted"]].agg(["mean","std"]).T)

# Matriz de confusÃ£o agregada (soma)
agg_cm = np.sum(all_conf_matrices, axis=0)
plt.figure(figsize=(6,5))
sns.heatmap(agg_cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Negativo","Neutro","Positivo"], yticklabels=["Negativo","Neutro","Positivo"])
plt.title("Matriz de ConfusÃ£o (agregada sobre 5 folds)")
plt.xlabel("Predito")
plt.ylabel("Real")
plt.show()

# Salvar mÃ©tricas
df_folds.to_csv("bert_bilstm_attn_metrics_per_fold.csv", index=False)
print("Salvo bert_bilstm_attn_metrics_per_fold.csv")

# =========================
# CÃ©lula 7 â€” Curva ROC Multiclasse Agregada (adaptada do modelo large)
# =========================
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import torch.nn.functional as F

print("\nGerando Curva ROC Multiclasse Agregada...")

# Converter listas acumuladas em arrays numpy
all_labels = np.array(all_labels)
all_probs = np.array(all_probs)

# Binarizar os rÃ³tulos para ROC multiclasses
y_bin = label_binarize(all_labels, classes=classes)
n_classes = y_bin.shape[1]

# Calcular FPR, TPR e AUC por classe
fpr, tpr, roc_auc = {}, {}, {}
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], all_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Calcular mÃ©dia macro (mÃ©dia das curvas)
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes
roc_auc["macro"] = auc(all_fpr, mean_tpr)

# ðŸŽ¨ Plotar curvas ROC com mesmo estilo do modelo large
plt.figure(figsize=(8,6))
colors = ["red", "gold", "green"]
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f"{labels_names[i]} (Ã¡rea = {roc_auc[i]:.2f})")
plt.plot(all_fpr, mean_tpr, color="navy", linestyle="--",
         label=f"MÃ©dia Macro (Ã¡rea = {roc_auc['macro']:.2f})", lw=2)
plt.plot([0, 1], [0, 1], "k--", lw=1)

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Curva ROC Multiclasse Agregada (5 Folds) â€” BiLSTM Adaptada")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()